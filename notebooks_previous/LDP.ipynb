{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns  # For heatmaps\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTEENN\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from scipy.stats import pointbiserialr  # For point-biserial correlation\n",
    "from pure_ldp.frequency_oracles.direct_encoding import DEClient\n",
    "from pure_ldp.frequency_oracles.local_hashing import LHClient\n",
    "from pure_ldp.frequency_oracles.rappor import RAPPORClient, RAPPORServer\n",
    "from pure_ldp.core import generate_hash\n",
    "from pure_ldp.core.prob_simplex import project_probability_simplex\n",
    "import random\n",
    "import sys\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression, Lasso\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b3d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear carpetas para resultados y figuras\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Crear subcarpetas organizadas para las figuras\n",
    "figures_base = 'figures'\n",
    "os.makedirs(figures_base, exist_ok=True)\n",
    "\n",
    "# Subcarpetas para cada método de privacidad\n",
    "privacy_methods = ['Direct_Encoding', 'OLH', 'RAPPOR']\n",
    "for method in privacy_methods + ['Original']:\n",
    "    method_path = os.path.join(figures_base, method)\n",
    "    os.makedirs(method_path, exist_ok=True)\n",
    "    # Subcarpetas para cada nivel de privacidad (Original y cada epsilon)\n",
    "    subfolders = ['Original'] + [f'eps_{eps}' for eps in [5, 3, 1, 0.5, 0.1]]\n",
    "    for subfolder in subfolders:\n",
    "        os.makedirs(os.path.join(method_path, subfolder), exist_ok=True)\n",
    "\n",
    "# Cargar datos\n",
    "data_path = '../data/raw/bank-full.csv'\n",
    "df = pd.read_csv(data_path, sep=';')\n",
    "df['y'] = df['y'].map({'no': 0, 'yes': 1})\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "# Mejores parámetros encontrados\n",
    "best_params_rf = {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "\n",
    "# Número de iteraciones para promediar resultados\n",
    "n_iterations = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff549f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función para mapear variables categóricas binarias y meses\n",
    "def map_binary_columns(df):\n",
    "    df['default'] = df['default'].map({'no': 0, 'yes': 1})\n",
    "    df['housing'] = df['housing'].map({'no': 0, 'yes': 1})\n",
    "    df['loan'] = df['loan'].map({'no': 0, 'yes': 1})\n",
    "    df['month'] = df['month'].map({\n",
    "        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n",
    "        'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "        'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Función para convertir variables numéricas en categóricas\n",
    "def convert_numerics_to_categories(df):\n",
    "    bins_age = [18, 30, 40, 50, 60, 95]\n",
    "    labels_age = ['18-30', '31-40', '41-50', '51-60', '61+']\n",
    "    df['age'] = pd.cut(df['age'], bins=bins_age, labels=labels_age, include_lowest=True)\n",
    "\n",
    "    bins_balance = [-8019, 0, 72, 448, 1428, 102127]\n",
    "    labels_balance = ['negative', '0-72', '73-448', '449-1428', '1429+']\n",
    "    df['balance'] = pd.cut(df['balance'], bins=bins_balance, labels=labels_balance, include_lowest=True)\n",
    "\n",
    "    bins_day = [1, 7, 14, 21, 31]\n",
    "    labels_day = ['1-7', '8-14', '15-21', '22-31']\n",
    "    df['day'] = pd.cut(df['day'], bins=bins_day, labels=labels_day, include_lowest=True)\n",
    "\n",
    "    bins_duration = [0, 103, 180, 319, 4918]\n",
    "    labels_duration = ['0-103', '104-180', '181-319', '320+']\n",
    "    df['duration'] = pd.cut(df['duration'], bins=bins_duration, labels=labels_duration, include_lowest=True)\n",
    "\n",
    "    bins_campaign = [1, 2, 3, 10, 63]\n",
    "    labels_campaign = ['1', '2-3', '4-10', '11+']\n",
    "    df['campaign'] = pd.cut(df['campaign'], bins=bins_campaign, labels=labels_campaign, include_lowest=True)\n",
    "\n",
    "    bins_pdays = [-1, 0, 30, 90, 871]\n",
    "    labels_pdays = ['no_contact', '0-30', '31-90', '91+']\n",
    "    df['pdays'] = pd.cut(df['pdays'], bins=bins_pdays, labels=labels_pdays, include_lowest=True)\n",
    "\n",
    "    bins_previous = [0, 1, 2, 5, 275]\n",
    "    labels_previous = ['0', '1-2', '3-5', '6+']\n",
    "    df['previous'] = pd.cut(df['previous'], bins=bins_previous, labels=labels_previous, include_lowest=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Función para mapear variables categóricas a valores numéricos\n",
    "def map_categorical_columns(df, categorical_columns):\n",
    "    mappings = {}\n",
    "    for col in categorical_columns:\n",
    "        unique_values = df[col].unique()\n",
    "        mappings[col] = {value: idx for idx, value in enumerate(unique_values) if pd.notna(value)}\n",
    "        df[col] = df[col].map(mappings[col])\n",
    "    return df, mappings\n",
    "\n",
    "# Funciones para aplicar DE, OLH y RAPPOR\n",
    "def apply_de(df, epsilon, categorical_columns):\n",
    "    df_de = df.copy()\n",
    "    for col in categorical_columns:\n",
    "        de = DEClient(epsilon=epsilon, d=df[col].nunique())\n",
    "        df_de[col] = df_de[col].apply(lambda x: de.privatise(int(x)))\n",
    "    return df_de\n",
    "\n",
    "def apply_olh(df, epsilon, categorical_columns):\n",
    "    df_olh = df.copy()\n",
    "    for col in categorical_columns:\n",
    "        d = df[col].nunique()\n",
    "        olh = LHClient(epsilon=epsilon, d=d, use_olh=True)\n",
    "        df_olh[col] = df_olh[col].apply(lambda x: olh.privatise(int(x))[0] % d)\n",
    "    return df_olh\n",
    "\n",
    "class FixedRAPPORClient(RAPPORClient):\n",
    "    def _perturb(self, data):\n",
    "        perturbed_data = data.copy()\n",
    "        for i, bit in enumerate(perturbed_data):\n",
    "            u = random.random()\n",
    "            if bit == 1:\n",
    "                if u < 0.5 * self.f:\n",
    "                    perturbed_data[i] = 0\n",
    "            else:\n",
    "                if u < 0.5 * self.f:\n",
    "                    perturbed_data[i] = 1\n",
    "        return perturbed_data\n",
    "\n",
    "class FixedRAPPORServer(RAPPORServer):\n",
    "    def _update_estimates(self):\n",
    "        y = self._create_y()\n",
    "        X = self._create_X()\n",
    "        if self.reg_const == 0:\n",
    "            model = LinearRegression(positive=True, fit_intercept=False)\n",
    "        else:\n",
    "            model = ElasticNet(positive=True, alpha=self.reg_const,\n",
    "                               l1_ratio=0, fit_intercept=False,\n",
    "                               max_iter=100000)\n",
    "        if self.d > 1000 or self.lasso:\n",
    "            lasso_model = Lasso(alpha=0.8, positive=True)\n",
    "            lasso_model.fit(X, y)\n",
    "            indexes = np.nonzero(lasso_model.coef_)[0]\n",
    "            X_red = X[:, indexes]\n",
    "            model.fit(X_red, y)\n",
    "            self.estimated_data[indexes] = model.coef_ * self.num_of_cohorts\n",
    "        else:\n",
    "            model.fit(X, y)\n",
    "            self.estimated_data = model.coef_ * self.num_of_cohorts\n",
    "\n",
    "def apply_rappor(df, epsilon, categorical_columns):\n",
    "    x = df.copy()\n",
    "    for col in categorical_columns:\n",
    "        d = x[col].nunique()\n",
    "        m = 128\n",
    "        k = 2\n",
    "        num_of_cohorts = 8\n",
    "        temp_client = FixedRAPPORClient(f=0.5, m=m, hash_funcs=[[]])\n",
    "        f = temp_client.convert_eps_to_f(epsilon)\n",
    "        hash_funcs = []\n",
    "        for i in range(num_of_cohorts):\n",
    "            cohort_hashes = [generate_hash(m, random.randint(0, sys.maxsize)) for _ in range(k)]\n",
    "            hash_funcs.append(cohort_hashes)\n",
    "        rappor_client = FixedRAPPORClient(f=f, m=m, hash_funcs=hash_funcs, num_of_cohorts=num_of_cohorts)\n",
    "        rappor_server = FixedRAPPORServer(f=f, m=m, k=k, d=d, num_of_cohorts=num_of_cohorts)\n",
    "        bit_vectors = []\n",
    "        for i in range(len(x)):\n",
    "            value = int(x[col].iloc[i])\n",
    "            bv = rappor_client.privatise(value)\n",
    "            bit_vectors.append(bv)\n",
    "        for bv in bit_vectors:\n",
    "            rappor_server.aggregate(bv)\n",
    "        freqs = {}\n",
    "        for i in range(d):\n",
    "            freqs[i] = max(rappor_server.estimate(i), 0)\n",
    "        freqs_array = np.array([freqs[i] for i in range(d)])\n",
    "        normalized_freqs = project_probability_simplex(freqs_array)\n",
    "        mapping = {i: normalized_freqs[i] for i in range(d)}\n",
    "        x[col] = x[col].map(mapping)\n",
    "    return x\n",
    "\n",
    "# Función para generar heatmaps de correlación entre features y target\n",
    "def plot_correlation_heatmap(X, y, title, filepath):\n",
    "    df_combined = X.copy()\n",
    "    df_combined['y'] = y\n",
    "    correlations = {}\n",
    "    for col in X.columns:\n",
    "        if df_combined[col].isna().any():\n",
    "            df_combined[col] = df_combined[col].fillna(0)\n",
    "        corr, _ = pointbiserialr(df_combined['y'], df_combined[col])\n",
    "        correlations[col] = corr\n",
    "    corr_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Correlation with y'])\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.heatmap(corr_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, cbar_kws={'label': 'Correlation'})\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Target (y)')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Nueva función para generar matrices de conteo (heatmaps) de feature vs target\n",
    "def plot_feature_target_count_matrices(X_original, X_privatized, y, categorical_columns, cat_mappings, method, model_name):\n",
    "    # Usar los datos originales (antes de mapear a numéricos) para obtener las categorías originales\n",
    "    df_original = X_original.copy()\n",
    "    df_original['y'] = y\n",
    "    # Usar los datos privatizados para generar las matrices\n",
    "    df_privatized = X_privatized.copy()\n",
    "    df_privatized['y'] = y\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        # Obtener las categorías originales del feature\n",
    "        categories = list(cat_mappings[col].keys())\n",
    "        # Crear una matriz de conteo: filas son las categorías del target (0, 1), columnas son las categorías del feature\n",
    "        count_matrix = pd.crosstab(df_privatized['y'], df_privatized[col])\n",
    "        # Asegurarse de que todas las categorías estén presentes en las columnas\n",
    "        count_matrix = count_matrix.reindex(columns=range(len(categories)), fill_value=0)\n",
    "        # Renombrar las columnas con las categorías originales\n",
    "        count_matrix.columns = categories\n",
    "        # Asegurarse de que el índice (y) tenga las etiquetas correctas\n",
    "        count_matrix.index = ['y=0', 'y=1']\n",
    "\n",
    "        # Crear el heatmap\n",
    "        plt.figure(figsize=(max(6, len(categories) * 0.5), 3))\n",
    "        sns.heatmap(count_matrix, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})\n",
    "        plt.title(f'Count Matrix: {col} vs Target ({method}, {model_name})')\n",
    "        plt.xlabel(f'{col} Categories')\n",
    "        plt.ylabel('Target (y)')\n",
    "        plt.tight_layout()\n",
    "        # Guardar en la carpeta correspondiente\n",
    "        filepath = os.path.join('figures', method, model_name, f'count_matrix_{col}.png')\n",
    "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# Procesamiento completo (modificado para devolver el modelo entrenado y las características seleccionadas)\n",
    "def process_data(X, y, categorical_columns, method='none', epsilon=None):\n",
    "    x = X.copy()\n",
    "    if method == 'de' and epsilon:\n",
    "        x = apply_de(x, epsilon, categorical_columns)\n",
    "    elif method == 'olh' and epsilon:\n",
    "        x = apply_olh(x, epsilon, categorical_columns)\n",
    "    elif method == 'rappor' and epsilon:\n",
    "        x = apply_rappor(x, epsilon, categorical_columns)\n",
    "    \n",
    "    x = pd.get_dummies(x, columns=categorical_columns, drop_first=True, dtype='int64')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "    smoteenn = SMOTEENN(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smoteenn.fit_resample(X_train, y_train)\n",
    "    rf = RandomForestClassifier(**best_params_rf, random_state=42)\n",
    "    boruta = BorutaPy(rf, n_estimators='auto', verbose=0)\n",
    "    boruta.fit(X_train_resampled.values, y_train_resampled.values)\n",
    "    selected_features = X_train_resampled.columns[boruta.support_].tolist()\n",
    "    \n",
    "    # Entrenar el modelo final con las características seleccionadas\n",
    "    model = RandomForestClassifier(**best_params_rf, random_state=42)\n",
    "    model.fit(X_train_resampled[selected_features], y_train_resampled)\n",
    "    \n",
    "    return X_train_resampled[selected_features], X_test[selected_features], y_train_resampled, y_test, model, selected_features\n",
    "\n",
    "# Entrenar y evaluar modelo (modificado para devolver el modelo)\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    actual_negatives, actual_positives = conf_matrix[0].sum(), conf_matrix[1].sum()\n",
    "    false_positive_pct = conf_matrix[0][1] / actual_negatives if actual_negatives > 0 else 0\n",
    "    false_negative_pct = conf_matrix[1][0] / actual_positives if actual_positives > 0 else 0\n",
    "    return {\n",
    "        'ROC AUC': roc_auc_score(y_test, y_prob),\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'Type I Error': false_positive_pct,\n",
    "        'Type II Error': false_negative_pct\n",
    "    }\n",
    "\n",
    "# Función para calcular la importancia de las características y guardarlas\n",
    "def save_feature_importance(model, selected_features, method, model_name):\n",
    "    importances = model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    filepath = os.path.join('results', f'feature_importance_{method}_{model_name}.csv')\n",
    "    feature_importance_df.to_csv(filepath, index=False)\n",
    "    return feature_importance_df\n",
    "\n",
    "# Función para ejecutar múltiples iteraciones y calcular estadísticas (modificado para manejar importancia de características)\n",
    "def run_iterations(X, y, X_original, categorical_columns, cat_mappings, method, epsilon, n_iterations):\n",
    "    results = []\n",
    "    feature_importances = []\n",
    "    for _ in range(n_iterations):\n",
    "        X_train, X_test, y_train, y_test, model, selected_features = process_data(\n",
    "            X, y, categorical_columns, method=method, epsilon=epsilon\n",
    "        )\n",
    "        result = evaluate_model(X_train, y_train, X_test, y_test, model)\n",
    "        results.append(result)\n",
    "        # Calcular y almacenar la importancia de las características\n",
    "        feature_importance_df = save_feature_importance(\n",
    "            model, selected_features, method, 'Original' if method == 'none' else f'eps_{epsilon}'\n",
    "        )\n",
    "        feature_importances.append(feature_importance_df)\n",
    "    return pd.DataFrame(results), feature_importances\n",
    "\n",
    "# Función para calcular estadísticas (promedio, mínimo, máximo)\n",
    "def compute_statistics(df):\n",
    "    stats = {\n",
    "        'mean': df.mean(),\n",
    "        'min': df.min(),\n",
    "        'max': df.max()\n",
    "    }\n",
    "    return stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499149da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas las columnas son ahora categóricas\n",
    "all_categorical_columns = ['default', 'housing', 'loan', 'job', 'marital', 'education', 'contact', 'poutcome',\n",
    "                           'age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "# Epsilons y resultados\n",
    "epsilons = [5, 3, 1, 0.5, 0.1]\n",
    "results_de_stats = {}\n",
    "results_olh_stats = {}\n",
    "results_rappor_stats = {}\n",
    "feature_importances_de = {}\n",
    "feature_importances_olh = {}\n",
    "feature_importances_rappor = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a186ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar datos originales\n",
    "X_original = X.copy()  # Guardar una copia del original para los heatmaps de conteo\n",
    "X = map_binary_columns(X)\n",
    "X = convert_numerics_to_categories(X)\n",
    "X, cat_mappings = map_categorical_columns(X, all_categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ebf7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando heatmap para datos originales...\n",
      "Generando matrices de conteo para datos originales...\n",
      "Generando heatmap para Direct Encoding (ε=5)...\n",
      "Generando matrices de conteo para Direct Encoding (ε=5)...\n",
      "Generando heatmap para OLH (ε=5)...\n",
      "Generando matrices de conteo para OLH (ε=5)...\n",
      "Generando heatmap para RAPPOR (ε=5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.029e+07, tolerance: 8.171e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.111e+08, tolerance: 2.227e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.528e+07, tolerance: 1.917e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.237e+08, tolerance: 2.487e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.092e+07, tolerance: 1.432e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.548e+07, tolerance: 1.313e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.789e+07, tolerance: 1.372e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.280e+08, tolerance: 2.577e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.673e+08, tolerance: 3.358e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:5405: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando matrices de conteo para RAPPOR (ε=5)...\n",
      "Generando heatmap para Direct Encoding (ε=3)...\n",
      "Generando matrices de conteo para Direct Encoding (ε=3)...\n",
      "Generando heatmap para OLH (ε=3)...\n",
      "Generando matrices de conteo para OLH (ε=3)...\n",
      "Generando heatmap para RAPPOR (ε=3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.472e+08, tolerance: 4.944e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.388e+08, tolerance: 2.779e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.036e+07, tolerance: 8.210e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.148e+08, tolerance: 2.321e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.628e+07, tolerance: 1.926e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.309e+08, tolerance: 2.627e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.750e+08, tolerance: 3.500e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.033e+07, tolerance: 1.410e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.626e+07, tolerance: 1.143e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.714e+07, tolerance: 1.346e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.269e+08, tolerance: 2.544e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.799e+08, tolerance: 3.606e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:5405: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando matrices de conteo para RAPPOR (ε=3)...\n",
      "Generando heatmap para Direct Encoding (ε=1)...\n",
      "Generando matrices de conteo para Direct Encoding (ε=1)...\n",
      "Generando heatmap para OLH (ε=1)...\n",
      "Generando matrices de conteo para OLH (ε=1)...\n",
      "Generando heatmap para RAPPOR (ε=1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.390e+08, tolerance: 2.780e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.967e+08, tolerance: 3.938e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.078e+07, tolerance: 1.038e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.152e+07, tolerance: 1.632e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.477e+07, tolerance: 1.498e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.962e+08, tolerance: 3.934e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+08, tolerance: 4.262e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando matrices de conteo para RAPPOR (ε=1)...\n",
      "Generando heatmap para Direct Encoding (ε=0.5)...\n",
      "Generando matrices de conteo para Direct Encoding (ε=0.5)...\n",
      "Generando heatmap para OLH (ε=0.5)...\n",
      "Generando matrices de conteo para OLH (ε=0.5)...\n",
      "Generando heatmap para RAPPOR (ε=0.5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.650e+08, tolerance: 3.302e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.918e+07, tolerance: 1.790e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.671e+08, tolerance: 3.350e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.499e+08, tolerance: 3.000e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.146e+08, tolerance: 4.309e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.196e+08, tolerance: 2.405e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.126e+08, tolerance: 2.255e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.173e+08, tolerance: 2.347e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.669e+08, tolerance: 3.341e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.238e+08, tolerance: 4.482e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.556e+08, tolerance: 5.124e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.964648227267553), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando matrices de conteo para RAPPOR (ε=0.5)...\n",
      "Generando heatmap para Direct Encoding (ε=0.1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:5405: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando matrices de conteo para Direct Encoding (ε=0.1)...\n",
      "Generando heatmap para OLH (ε=0.1)...\n",
      "Generando matrices de conteo para OLH (ε=0.1)...\n",
      "Generando heatmap para RAPPOR (ε=0.1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.107e+09, tolerance: 4.214e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.815e+09, tolerance: 3.643e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.881e+09, tolerance: 3.767e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.993e+09, tolerance: 4.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.832e+09, tolerance: 3.665e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.879e+09, tolerance: 3.759e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.047e+09, tolerance: 4.102e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\pure_ldp\\core\\_freq_oracle_server.py:76: RuntimeWarning: High privacy has been detected (epsilon = 0.16002133845479682), estimations may be highly inaccurate on small datasets\n",
      "  warnings.warn(\"High privacy has been detected (epsilon = \" + str(self.epsilon) +\n",
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\.venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:5405: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando matrices de conteo para RAPPOR (ε=0.1)...\n"
     ]
    }
   ],
   "source": [
    "# Generar heatmap de correlación y matrices de conteo para los datos originales\n",
    "print(\"Generando heatmap para datos originales...\")\n",
    "plot_correlation_heatmap(\n",
    "    X, y, 'Correlation of Features with Target (Original Data)',\n",
    "    os.path.join('figures', 'Original', 'Original', 'heatmap_correlation.png')\n",
    ")\n",
    "print(\"Generando matrices de conteo para datos originales...\")\n",
    "plot_feature_target_count_matrices(\n",
    "    X_original, X, y, all_categorical_columns, cat_mappings, 'Original', 'Original'\n",
    ")\n",
    "# Generar heatmaps y matrices de conteo para cada método y epsilon después de la privatización\n",
    "for epsilon in epsilons:\n",
    "    # Direct Encoding\n",
    "    print(f\"Generando heatmap para Direct Encoding (ε={epsilon})...\")\n",
    "    X_de = apply_de(X.copy(), epsilon, all_categorical_columns)\n",
    "    plot_correlation_heatmap(\n",
    "        X_de, y, f'Correlation of Features with Target (Direct Encoding, ε={epsilon})',\n",
    "        os.path.join('figures', 'Direct_Encoding', f'eps_{epsilon}', 'heatmap_correlation.png')\n",
    "    )\n",
    "    print(f\"Generando matrices de conteo para Direct Encoding (ε={epsilon})...\")\n",
    "    plot_feature_target_count_matrices(\n",
    "        X_original, X_de, y, all_categorical_columns, cat_mappings, 'Direct_Encoding', f'eps_{epsilon}'\n",
    "    )\n",
    "    \n",
    "    # OLH\n",
    "    print(f\"Generando heatmap para OLH (ε={epsilon})...\")\n",
    "    X_olh = apply_olh(X.copy(), epsilon, all_categorical_columns)\n",
    "    plot_correlation_heatmap(\n",
    "        X_olh, y, f'Correlation of Features with Target (OLH, ε={epsilon})',\n",
    "        os.path.join('figures', 'OLH', f'eps_{epsilon}', 'heatmap_correlation.png')\n",
    "    )\n",
    "    print(f\"Generando matrices de conteo para OLH (ε={epsilon})...\")\n",
    "    plot_feature_target_count_matrices(\n",
    "        X_original, X_olh, y, all_categorical_columns, cat_mappings, 'OLH', f'eps_{epsilon}'\n",
    "    )\n",
    "    \n",
    "    # RAPPOR\n",
    "    print(f\"Generando heatmap para RAPPOR (ε={epsilon})...\")\n",
    "    X_rappor = apply_rappor(X.copy(), epsilon, all_categorical_columns)\n",
    "    plot_correlation_heatmap(\n",
    "        X_rappor, y, f'Correlation of Features with Target (RAPPOR, ε={epsilon})',\n",
    "        os.path.join('figures', 'RAPPOR', f'eps_{epsilon}', 'heatmap_correlation.png')\n",
    "    )\n",
    "    print(f\"Generando matrices de conteo para RAPPOR (ε={epsilon})...\")\n",
    "    plot_feature_target_count_matrices(\n",
    "        X_rappor, X_rappor, y, all_categorical_columns, cat_mappings, 'RAPPOR', f'eps_{epsilon}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02172e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo sin privacidad (Original)\n",
    "results_original, feature_importances_original = run_iterations(\n",
    "    X, y, X_original, all_categorical_columns, cat_mappings, method='none', epsilon=None, n_iterations=n_iterations\n",
    ")\n",
    "results_de_stats['Original'] = compute_statistics(results_original)\n",
    "results_olh_stats['Original'] = results_de_stats['Original']\n",
    "results_rappor_stats['Original'] = results_de_stats['Original']\n",
    "feature_importances_de['Original'] = feature_importances_original\n",
    "feature_importances_olh['Original'] = feature_importances_original\n",
    "feature_importances_rappor['Original'] = feature_importances_original\n",
    "\n",
    "# Ejecutar iteraciones para cada método y epsilon\n",
    "for epsilon in epsilons:\n",
    "    # Direct Encoding\n",
    "    results_de, feature_importances_de_eps = run_iterations(\n",
    "        X, y, X_original, all_categorical_columns, cat_mappings, method='de', epsilon=epsilon, n_iterations=n_iterations\n",
    "    )\n",
    "    results_de_stats[f'ε={epsilon}'] = compute_statistics(results_de)\n",
    "    feature_importances_de[f'ε={epsilon}'] = feature_importances_de_eps\n",
    "    \n",
    "    # OLH\n",
    "    results_olh, feature_importances_olh_eps = run_iterations(\n",
    "        X, y, X_original, all_categorical_columns, cat_mappings, method='olh', epsilon=epsilon, n_iterations=n_iterations\n",
    "    )\n",
    "    results_olh_stats[f'ε={epsilon}'] = compute_statistics(results_olh)\n",
    "    feature_importances_olh[f'ε={epsilon}'] = feature_importances_olh_eps\n",
    "    \n",
    "    # RAPPOR\n",
    "    results_rappor, feature_importances_rappor_eps = run_iterations(\n",
    "        X, y, X_original, all_categorical_columns, cat_mappings, method='rappor', epsilon=epsilon, n_iterations=n_iterations\n",
    "    )\n",
    "    results_rappor_stats[f'ε={epsilon}'] = compute_statistics(results_rappor)\n",
    "    feature_importances_rappor[f'ε={epsilon}'] = feature_importances_rappor_eps\n",
    "\n",
    "# Guardar resultados en CSVs\n",
    "def save_results_to_csv(stats_dict, method_name):\n",
    "    data = {}\n",
    "    for model, stats in stats_dict.items():\n",
    "        data[f'{model} (mean)'] = stats['mean']\n",
    "        data[f'{model} (min)'] = stats['min']\n",
    "        data[f'{model} (max)'] = stats['max']\n",
    "    df = pd.DataFrame(data).round(4)\n",
    "    df.to_csv(f'results/{method_name}_results.csv')\n",
    "    return df\n",
    "\n",
    "print(\"Guardando resultados en CSVs...\")\n",
    "results_de_df = save_results_to_csv(results_de_stats, 'Direct_Encoding')\n",
    "results_olh_df = save_results_to_csv(results_olh_stats, 'OLH')\n",
    "results_rappor_df = save_results_to_csv(results_rappor_stats, 'RAPPOR')\n",
    "# Función para graficar con bigotes verticales (métricas en el eje X, modelos en la leyenda)\n",
    "def plot_results_with_whiskers(stats_dict, title, colors):\n",
    "    metrics = list(stats_dict['Original']['mean'].keys())\n",
    "    models = list(stats_dict.keys())\n",
    "    n_metrics = len(metrics)\n",
    "    n_models = len(models)\n",
    "    \n",
    "    plt.figure(figsize=(11, 6))\n",
    "    x_positions = np.arange(n_metrics) * 1.3  # Espaciado entre las métricas\n",
    "    \n",
    "    for model_idx, model in enumerate(models):\n",
    "        means = []\n",
    "        mins = []\n",
    "        maxs = []\n",
    "        for metric in metrics:\n",
    "            means.append(stats_dict[model]['mean'][metric])\n",
    "            mins.append(stats_dict[model]['min'][metric])\n",
    "            maxs.append(stats_dict[model]['max'][metric])\n",
    "        \n",
    "        plt.scatter(x_positions + (model_idx - (n_models-1)/2) * 0.15, means, \n",
    "                    color=colors[model_idx], label=model, s=85) \n",
    "        for metric_idx in range(n_metrics):\n",
    "            plt.vlines(x_positions[metric_idx] + (model_idx - (n_models-1)/2) * 0.15, \n",
    "                       mins[metric_idx], maxs[metric_idx], \n",
    "                       color=colors[model_idx], linestyle='-', linewidth=1)\n",
    "    \n",
    "    plt.xticks(x_positions, metrics, rotation=45, fontdict={'fontsize': 11})\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.title(title, fontdict={'fontsize': 16, 'weight': 'bold'})\n",
    "    plt.xlabel('Metrics', fontdict={'fontsize': 13})\n",
    "    plt.ylabel('Value', fontdict={'fontsize': 13})\n",
    "    plt.legend(title='Models', bbox_to_anchor=(1, 1), loc='upper left', fontsize=11, title_fontsize=13)  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Colores para los modelos (Original, ε=5, ε=3, ε=1, ε=0.5, ε=0.1)\n",
    "colors = ['#0d4e9e', '#ffc520', '#7b9ca0', '#242624', '#cc7b4f', '#7764B4']\n",
    "\n",
    "# Graficar resultados con bigotes verticales\n",
    "plot_results_with_whiskers(results_de_stats, 'Comparison of Metrics for Different $\\epsilon$ Values in Direct Encoding', colors)\n",
    "plot_results_with_whiskers(results_olh_stats, 'Comparison of Metrics for Different $\\epsilon$ Values in OLH', colors)\n",
    "plot_results_with_whiskers(results_rappor_stats, 'Comparison of Metrics for Different $\\epsilon$ Values in RAPPOR', colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
