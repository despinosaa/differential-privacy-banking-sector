{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8da1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.combine import SMOTEENN\n",
    "from boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a2e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('C:/Users/danie/OneDrive/Documentos/1 UNIANDES/10 semestre/Tesis/differential-privacy-banking-sector/data/processed/bank-processed.csv')\n",
    "X = data.drop(columns=['y'])\n",
    "y = data['y']\n",
    "\n",
    "# Normalize numeric columns\n",
    "numeric_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "X[numeric_cols] = MinMaxScaler().fit_transform(X[numeric_cols])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5483517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTEENN\n",
    "X_resample, y_resample = SMOTEENN(random_state=42).fit_resample(X_train, y_train)\n",
    "X_resample = pd.DataFrame(X_resample, columns=X.columns)\n",
    "y_resample = pd.Series(y_resample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121636e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\OneDrive\\Documentos\\1 UNIANDES\\10 semestre\\Tesis\\differential-privacy-banking-sector\\cdp\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Boruta feature selection\n",
    "rf = xgb.XGBClassifier(eval_metric='logloss')\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=42)\n",
    "feat_selector.fit(X_resample.values, y_resample.values.ravel())\n",
    "X_filtered = X.columns[feat_selector.support_].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered = X_resample[X_filtered].values\n",
    "X_test_filtered = X_test[X_filtered].values\n",
    "y_train_filtered = y_resample.values\n",
    "y_test_filtered = y_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304aa9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "input_size = len(X_filtered)\n",
    "hidden_units = 64\n",
    "hidden_layers = 2\n",
    "dropout_rate = 0.2\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "l2_norm_clip = 1.0\n",
    "default_noise_multiplier = 1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP parameters\n",
    "def compute_privacy_budget(n, batch_size, noise_multiplier, epochs, delta=1e-5):\n",
    "    try:\n",
    "        return compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
    "            n=n, batch_size=batch_size, noise_multiplier=noise_multiplier, epochs=epochs, delta=delta)[0]\n",
    "    except:\n",
    "        return float('inf')\n",
    "\n",
    "def create_model(input_size, hidden_units, hidden_layers, dropout_rate, learning_rate, num_microbatches, l2_norm_clip, noise_multiplier, use_dp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, activation='relu', input_shape=(input_size,)))\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(Dense(hidden_units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    if use_dp:\n",
    "        optimizer = DPKerasSGDOptimizer(\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            num_microbatches=num_microbatches,\n",
    "            learning_rate=learning_rate\n",
    "        )\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test, batch_size, epochs, learning_rate, use_dp, noise_multiplier, l2_norm_clip, use_early_stopping):\n",
    "    num_microbatches = batch_size\n",
    "    model = create_model(\n",
    "        input_size, hidden_units, hidden_layers, dropout_rate,\n",
    "        learning_rate, num_microbatches, l2_norm_clip,\n",
    "        noise_multiplier, use_dp\n",
    "    )\n",
    "\n",
    "    callbacks = []\n",
    "    if use_early_stopping:\n",
    "        callbacks.append(EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True))\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    y_pred_prob = model.predict(X_test, batch_size=batch_size).flatten()\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    return y_pred_prob, y_pred\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_pred_prob):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return {\n",
    "        'ROC AUC': roc_auc_score(y_true, y_pred_prob),\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1 Score': f1_score(y_true, y_pred),\n",
    "        'Type I Error': cm[0][1] / cm[0].sum() if cm[0].sum() > 0 else 0,\n",
    "        'Type II Error': cm[1][0] / cm[1].sum() if cm[1].sum() > 0 else 0\n",
    "    }\n",
    "\n",
    "def run_experiment(X_train_data, y_train_data, batch_size, learning_rate, noise_multiplier, l2_norm_clip, use_dp=True, use_early_stopping=True):\n",
    "    eps = compute_privacy_budget(len(X_train_data), batch_size, noise_multiplier, epochs)\n",
    "    print(f\"Running: batch_size={batch_size}, lr={learning_rate}, noise={noise_multiplier}, clip={l2_norm_clip}, eps={eps:.2f}\")\n",
    "    y_prob, y_pred = train_model(\n",
    "        X_train_data, y_train_data, X_test_filtered, y_test_filtered,\n",
    "        batch_size=batch_size, epochs=epochs, learning_rate=learning_rate,\n",
    "        use_dp=use_dp, noise_multiplier=noise_multiplier, l2_norm_clip=l2_norm_clip,\n",
    "        use_early_stopping=use_early_stopping\n",
    "    )\n",
    "    results = evaluate_model(y_test_filtered, y_pred, y_prob)\n",
    "    results['Epsilon'] = eps\n",
    "    results['Batch Size'] = batch_size\n",
    "    results['Noise Multiplier'] = noise_multiplier\n",
    "    results['Learning Rate'] = learning_rate\n",
    "    results['Clipping Norm'] = l2_norm_clip\n",
    "    results['Sample Ratio'] = len(X_train_data) / len(X_train_filtered)\n",
    "    return results\n",
    "\n",
    "def grid_search_experiments(use_early_stopping=True):\n",
    "    batch_sizes = [16, 32, 64, 128]\n",
    "    noise_multipliers = [0.8, 1.1, 1.5, 2.0]\n",
    "    learning_rates = [0.001, 0.003, 0.005]\n",
    "    clip_norms = [0.5, 1.0, 2.0]\n",
    "    sample_ratios = [1.0, 0.5, 0.1, 0.05]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for ratio in sample_ratios:\n",
    "        n_samples = int(len(X_train_filtered) * ratio)\n",
    "        idx = np.random.choice(len(X_train_filtered), n_samples, replace=False)\n",
    "        X_sample = X_train_filtered[idx]\n",
    "        y_sample = y_train_filtered[idx]\n",
    "\n",
    "        # Run non-DP experiments for all batch sizes and learning rates\n",
    "        for batch_size in batch_sizes:\n",
    "            for lr in learning_rates:\n",
    "                res = run_experiment(\n",
    "                    X_sample, y_sample,\n",
    "                    batch_size=batch_size,\n",
    "                    learning_rate=lr,\n",
    "                    noise_multiplier=0.0,\n",
    "                    l2_norm_clip=0.0,\n",
    "                    use_dp=False,\n",
    "                    use_early_stopping=use_early_stopping\n",
    "                )\n",
    "                results.append(res)\n",
    "\n",
    "        # Run DP experiments for all combinations\n",
    "        for batch_size in batch_sizes:\n",
    "            for noise in noise_multipliers:\n",
    "                for lr in learning_rates:\n",
    "                    for clip in clip_norms:\n",
    "                        res = run_experiment(\n",
    "                            X_sample, y_sample,\n",
    "                            batch_size=batch_size,\n",
    "                            learning_rate=lr,\n",
    "                            noise_multiplier=noise,\n",
    "                            l2_norm_clip=clip,\n",
    "                            use_dp=True,\n",
    "                            use_early_stopping=use_early_stopping\n",
    "                        )\n",
    "                        results.append(res)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b981f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search with early stopping\n",
    "df_results = grid_search_experiments(use_early_stopping=True)\n",
    "df_results.round(3).to_csv('results/cdp_results_early_stopping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d21d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search without early stopping\n",
    "df_results_no_early_stopping = grid_search_experiments(use_early_stopping=False)\n",
    "df_results_no_early_stopping.round(3).to_csv('results/cdp_results_no_early_stopping.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
